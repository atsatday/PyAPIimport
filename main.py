import os
import json
import time
import re
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin


def load_dictionaries():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ª–æ–≤–∞—Ä–∏ –∏–∑ JSON —Ñ–∞–π–ª–∞"""
    try:
        with open('dictionaries.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
            return data if isinstance(data, list) else [data]
    except Exception as e:
        print(f"üö® –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ dictionaries.json: {e}")
        return []


def sanitize_filename(name):
    """–û—á–∏—â–∞–µ—Ç –∏–º—è –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –Ω–∞–∑–≤–∞–Ω–∏—è—Ö —Ñ–∞–π–ª–æ–≤"""
    return re.sub(r'[<>:"/\\|?*]', '', name).replace(' ', '_')[:50]


def get_all_words(dictionary_url):
    """–ü–æ–ª—É—á–∞–µ—Ç –≤—Å–µ —Å–ª–æ–≤–∞ –∏–∑ —Å–ª–æ–≤–∞—Ä—è"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'
    }

    try:
        response = requests.get(dictionary_url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å–ª–æ–≤–∞—Ä–µ–π
        word_elements = soup.select('div.article-link a, .name-list a, a.name-link, .list-group-item a, .word-item a')
        words = {elem.text.strip() for elem in word_elements if elem.text.strip()}

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –∞–ª—Ñ–∞–≤–∏—Ç–Ω—ã–π —É–∫–∞–∑–∞—Ç–µ–ª—å
        if len(words) < 10:
            alphabet_blocks = soup.find_all('div', class_=re.compile('alphabet|letters', re.I))
            for block in alphabet_blocks:
                letter_links = block.find_all('a', href=True)
                for link in letter_links:
                    letter_url = urljoin(dictionary_url, link['href'])
                    if letter_words := get_words_from_letter_page(letter_url):
                        words.update(letter_words)
                    time.sleep(1)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏

        return sorted(words) if words else None

    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å–ª–æ–≤–∞—Ä—è {dictionary_url}: {e}")
        return None


def get_words_from_letter_page(url):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–ª–æ–≤–∞ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –±—É–∫–≤—ã"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'
    }
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        return {item.text.strip() for item in soup.select('.word-list li, .article-item, .article-link a') if
                item.text.strip()}
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –±—É–∫–≤—ã: {e}")
        return None


def search_phrases(query, base_url, api_key, headers, pause_time):
    """–ü–æ–∏—Å–∫ —Ñ—Ä–∞–∑ —á–µ—Ä–µ–∑ API"""
    try:
        time.sleep(pause_time)
        response = requests.get(
            f"{base_url}/search",
            params={"q": query, "api_key": api_key},
            headers=headers,
            timeout=10
        )
        return response.json().get('result', {}).get('phrases', []) if response.status_code == 200 else []
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –¥–ª—è '{query}': {e}")
        return []


def get_phrase_description(phrase, base_url, api_key, headers, pause_time):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è —Ñ—Ä–∞–∑—ã —á–µ—Ä–µ–∑ API"""
    try:
        time.sleep(pause_time)
        response = requests.get(
            f"{base_url}/phrase",
            params={"q": phrase, "api_key": api_key},
            headers=headers,
            timeout=10
        )
        if response.status_code == 200:
            result = response.json().get('result', [{}])
            return result[0] if result else None
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –æ–ø–∏—Å–∞–Ω–∏—è –¥–ª—è '{phrase}': {e}")
    return None


def save_data(data, folder, filename):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–∞–π–ª"""
    try:
        os.makedirs(folder, exist_ok=True)
        filepath = os.path.join(folder, filename)

        if filename.endswith('.json'):
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        else:
            with open(filepath, 'w', encoding='utf-8') as f:
                if isinstance(data, list):
                    f.write('\n'.join(data))
                else:
                    f.write(str(data))

        return True
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è {filepath}: {e}")
        return False


def process_dictionary(dictionary, api_key=None):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω —Å–ª–æ–≤–∞—Ä—å"""
    print(f"\n{'=' * 60}")
    print(f"üìö –°–ª–æ–≤–∞—Ä—å: {dictionary.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}")
    print(f"üîó URL: {dictionary.get('url')}")

    if not dictionary.get('url'):
        print("‚è≠ –ü—Ä–æ–ø—É—Å–∫–∞–µ–º - –Ω–µ—Ç URL")
        return False

    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —Å–ª–æ–≤–∞—Ä—è
    folder_name = os.path.join("BaseSlovar", sanitize_filename(dictionary.get('title', 'Unnamed_Dictionary')))
    print(f"üìÅ –ü–∞–ø–∫–∞: {folder_name}")

    # –ü–æ–ª—É—á–∞–µ–º —Å–ª–æ–≤–∞ –∏–∑ —Å–ª–æ–≤–∞—Ä—è
    words = get_all_words(dictionary['url'])
    if not words:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ª–æ–≤–∞ –∏–∑ —Å–ª–æ–≤–∞—Ä—è")
        return False

    print(f"üî† –ù–∞–π–¥–µ–Ω–æ —Å–ª–æ–≤: {len(words)}")
    print("üìù –ü—Ä–∏–º–µ—Ä—ã:", ', '.join(words[:3]) + (', ...' if len(words) > 3 else ''))

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ —Å–ª–æ–≤–∞
    save_data(words, folder_name, 'all_words.txt')

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ API (–µ—Å–ª–∏ –µ—Å—Ç—å –∫–ª—é—á)
    if api_key:
        process_with_api(words, folder_name, dictionary['url'], api_key)
    else:
        # –ë–µ–∑ API –ø—Ä–æ—Å—Ç–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ª–æ–≤–∞ –ø–æ –æ—Ç–¥–µ–ª—å–Ω–æ—Å—Ç–∏
        words_dir = os.path.join(folder_name, 'words')
        os.makedirs(words_dir, exist_ok=True)
        for i, word in enumerate(words, 1):
            save_data(word, words_dir, f'word_{i}.txt')

    print(f"‚úÖ –ì–æ—Ç–æ–≤–æ! –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤: {folder_name}")
    return True


def process_with_api(words, folder_name, base_url, api_key):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–ª–æ–≤–∞ —á–µ—Ä–µ–∑ API"""
    print("\nüîç –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —á–µ—Ä–µ–∑ API...")
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    pause_time = 1  # 1 –∑–∞–ø—Ä–æ—Å –≤ —Å–µ–∫—É–Ω–¥—É

    # –°–æ–∑–¥–∞–µ–º –ø–æ–¥–ø–∞–ø–∫–∏ –¥–ª—è API —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    phrases_dir = os.path.join(folder_name, 'phrases')
    descriptions_dir = os.path.join(folder_name, 'descriptions')
    os.makedirs(phrases_dir, exist_ok=True)
    os.makedirs(descriptions_dir, exist_ok=True)

    api_base_url = base_url + "/api/v1-alpha" if not base_url.endswith('/') else base_url + "api/v1-alpha"

    for i, word in enumerate(words, 1):
        print(f"[{i}/{len(words)}] –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º: {word}")

        phrases = search_phrases(word, api_base_url, api_key, headers, pause_time)
        if not phrases:
            print(f"  –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ—Ä–∞–∑ –¥–ª—è '{word}'")
            continue

        phrases_info = []
        descriptions = []

        for phrase in phrases:
            if desc := get_phrase_description(phrase, api_base_url, api_key, headers, pause_time):
                phrases_info.append(desc)
                if 'description' in desc:
                    descriptions.append(desc['description'])

        if phrases_info:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º phrases –≤ JSON
            save_data(phrases_info, phrases_dir, f'phrases_{i}.json')
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º descriptions –≤ TXT
            save_data('\n\n'.join(descriptions), descriptions_dir, f'descriptions_{i}.txt')
            print(f"  –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ —Ñ—Ä–∞–∑: {len(phrases_info)}")
        else:
            print(f"  –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ–ø–∏—Å–∞–Ω–∏—è –¥–ª—è '{word}'")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ª–æ–≤–∞—Ä–µ–π...")
    dictionaries = load_dictionaries()

    if not dictionaries:
        print("‚ùå –ù–µ—Ç —Å–ª–æ–≤–∞—Ä–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        return

    # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—É—é –ø–∞–ø–∫—É
    os.makedirs("BaseSlovar", exist_ok=True)

    # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º API –∫–ª—é—á –æ–¥–∏–Ω —Ä–∞–∑
    api_key = input("–í–≤–µ–¥–∏—Ç–µ API-–∫–ª—é—á (–µ—Å–ª–∏ –Ω–µ—Ç, –Ω–∞–∂–º–∏—Ç–µ Enter): ").strip() or None

    print(f"\nüéØ –ù–∞–π–¥–µ–Ω–æ —Å–ª–æ–≤–∞—Ä–µ–π: {len(dictionaries)}")
    successful = 0

    for i, dictionary in enumerate(dictionaries, 1):
        print(f"\n{'=' * 30} –°–ª–æ–≤–∞—Ä—å {i}/{len(dictionaries)} {'=' * 30}")
        if process_dictionary(dictionary, api_key):
            successful += 1

    print(f"\n{'=' * 60}")
    print(f"üèÅ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {successful}/{len(dictionaries)}")
    print(f"üíæ –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–µ BaseSlovar")


if __name__ == "__main__":
    start_time = time.time()
    main()
    print(f"\n‚è± –û–±—â–µ–µ –≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã: {time.time() - start_time:.1f} —Å–µ–∫—É–Ω–¥")